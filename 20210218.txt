Unit 2 - Nonlinear Classification, Linear Regression and Colloborative filtering
--------------------------------------------------------------------------------
Empirical risk
--------------
R(0)=0.25*sum(loss(y(i)-0[i]x[i]))
0.25*loss(2 - [0,1,2]*[1,0,1]) + 0.25*loss(2.7 - [0,1,2]*[1,1,1]) +
0.25*loss(-0.7 - [0,1,2]*[1,1,-1]) + 0.25*loss(2 - [0,1,2]*[-1,1,1])

0.25*loss(0) + 0.25*loss(-0.7) +
0.25*loss(-1.7) + 0.25*loss(-1)

Hingle loss
-----------
0.25*1 + 0.25*1.7 +
0.25*2.7 + 0.25*2

= 1.85

Square loss
-----------
0 + 0.49/4 + 
2.89/4 + 1/4

Kernels as dot products
-----------------------
[x1, x2+x3] [x1', x2'+x3'] = x1x1' + x2x2' + x3x3' + x2x3' + x2'x3 

Kernel Composition Rules
------------------------
gamma = f * phi
Kb =  (f)(f')(phi)(phi') 

Topics covered
--------------
Linear Regression and Nonlinear Classification 
-Stochastic gradient descent method, closed form linear regression, regularization
-Kernel function algorithm, example for random forests