Unit 2 - Nonlinear Classification, Linear Regression and Colloborative filtering
--------------------------------------------------------------------------------
Empirical risk
--------------
R(0)=0.25*sum(loss(y(i)-0[i]x[i]))
0.25*loss(2 - [0,1,2]*[1,0,1]) + 0.25*loss(2.7 - [0,1,2]*[1,1,1]) +
0.25*loss(-0.7 - [0,1,2]*[1,1,-1]) + 0.25*loss(2 - [0,1,2]*[-1,1,1])

0.25*loss(0) + 0.25*loss(-0.3) +
0.25*loss(0.3) + 0.25*loss(-1)

Hingle loss
-----------
0.25*1 + 0.25*1.3 +
0.25*0.7 + 0.25*2

= 1.25

Square loss
-----------
0 + 0.09/8 + 
0.09/8 + 1/8

= 0.1475

Counting Dimensions of Feature Vectors
--------------------------------------
Variables = 150
Degree = 3
Dimensions = 
(n+k-1) 
(  k  )
=  (n+k-1)!
   ------
   (n-1)!k!
= 152!/(149!3!) = 152*151*150/6 = 573800


Kernels as dot products
-----------------------
[x1, x2+x3] [x1', x2'+x3'] = x1x1' + x2x2' + x3x3' + x2x3' + x2'x3 

Kernel Composition Rules
------------------------
gamma = f * phi
Kb =  (f)(f')(phi)(phi') 

Topics covered
--------------
Linear Regression and Nonlinear Classification 
-Stochastic gradient descent method, closed form linear regression, regularization
-Kernel function algorithm, example for random forests

Computing derivative of Regression Objective
--------------------------------------------
J = (Y-X)^2/2 + lambda*X^2/2
grad J = -(Y-X) + lambda*X
= (X-Y) + lambda*X
= X(1+lambda) - Y