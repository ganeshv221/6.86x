Reinforcement Learning
----------------------
Bellmans Equations
------------------
V*(s) = max(a)Q*(s,a) - Optimal value function or optimal expected reward from starting state s

Value function in Terms of Q function
-------------------------------------
Q*(s,a1) = 10
Q*(s,a2) = -1
Q*(s,a3) = 0
Q*(s,a4) = 11

V*(s) = max(a) Q*(s,a)
= Q*(s,a4) 
= 11

Q*(s,a) - Optimal expected reward from starting state s, given action a

Bellman Equation for Q function
-------------------------------
Q*(s,a1) = 10
Q*(s,a2) = -1
Q*(s,a3) = 0
Q*(s,a4) = 11

T(s,a1,s') = 1
R(s,a1,s') = 5
gamma = 0.5

Q*(s,a) = sum(s') T(s,a,s')*(R(s,a,s') + gamma*V*(s'))

sum(s') T(s,a,s') = 1 (total probabilities is 1) and  T(s,a,s') = 1 (given)
T(s,a,any other state) = 0

10 = T(s,a1,s')*(R(s,a1,s') + gamma*V*(s'))
10 = 1*(5 + 0.5*V*(s'))
V*(s') = 10

Value Ieration Update rule
--------------------------
V*k+1(s) = max(sum(s') T(s,a,s')*(R(s,a,s') + gamma*V*k(s')))

Value function Update
---------------------
gamma = 0.5
V*k+1(s) = [V*k(1) V*k(2) V*k(3) V*k(4) V*k(5)]
V*0(s) = [0 0 0 0 0]
T(s,a,s') = 1
V*1(s) = [R(1,'right',2) + 0.5*V*(s') R(2,'right',3) + 0.5*V*(s') R(3,'right',4) + 0.5*V*(s') R(4,'right',5) + 0.5*V*(s') R(5)]
V*1(s) = [V*0(1) V*0(2) V*0(3) V*0(4) V*0(5)] = [0 0 0 0 1]
V*2(s) = [V*1(1) V*1(2) V*1(3) V*1(4) V*1(5)] = [0 0 0 0.5 1]
V*3(s) = [V*3(1) V*3(2) V*3(3) V*3(4) V*3(5)] = [0 0 0.25 0.5 1]
V*4(s) = [V*4(1) V*4(2) V*4(3) V*4(4) V*4(5)] = [0 0.125 0.25 0.5 1]
V*5(s) = [V*5(1) V*5(2) V*5(3) V*5(4) V*5(5)] = [0.03125 0.125 0.25 0.5 1]

states - 5
actions - 1

Another Example of Value Iteration
----------------------------------
Case 1
------
a = {a1,a2,a3} = {'stay','left','right'}

Transition probabilities for given state, given action

P(1|1,stay) P(2|1,stay) P(3|1,stay) P(4|1,stay) P(5|1,stay)   
P(1|2,stay) P(2|2,stay) P(3|2,stay) P(4|2,stay) P(5|2,stay)
P(1|3,stay) P(2|3,stay) P(3|3,stay) P(4|3,stay) P(5|3,stay)
P(1|4,stay) P(2|4,stay) P(3|4,stay) P(4|4,stay) P(5|4,stay)
P(1|5,stay) P(2|5,stay) P(3|5,stay) P(4|5,stay) P(5|5,stay)

0.5  0.5  0    0    0
0.25 0.5  0.25 0    0
0    0.25 0.5  0.25 0
0    0    0.25 0.5  0.25
0    0    0    0.5  0.5

P(1|1,left) P(2|1,left) P(3|1,left) P(4|1,left) P(5|1,left)
P(1|2,left) P(2|2,left) P(3|2,left) P(4|2,left) P(5|2,left)
P(1|3,left) P(2|3,left) P(3|3,left) P(4|3,left) P(5|3,left)
P(1|4,left) P(2|4,left) P(3|4,left) P(4|4,left) P(5|4,left)
P(1|5,left) P(2|5,left) P(3|5,left) P(4|5,left) P(5|5,left)

0.5  0.5  0    0    0
0.33 0.67 0    0    0
0    0.33 0.67 0    0
0    0    0.33 0.67 0
0    0    0    0.33 0.67

P(1|1,right) P(2|1,right) P(3|1,right) P(4|1,right) P(5|1,right)
P(1|2,right) P(2|2,right) P(3|2,right) P(4|2,right) P(5|2,right)
P(1|3,right) P(2|3,right) P(3|3,right) P(4|3,right) P(5|3,right)
P(1|4,right) P(2|4,right) P(3|4,right) P(4|4,right) P(5|4,right)
P(1|5,right) P(2|5,right) P(3|5,right) P(4|5,right) P(5|5,right)

0.67 0.33 0    0    0   
0    0.67 0.33 0    0   
0    0    0.67 0.33 0
0    0    0    0.67 0.33
0    0    0    0.5  0.5

Initial value of state

V0 = [0 0 0 0 0]
R  = [0 0 0 0 1]

V*(k+1) = max(a) [sum(s') T(s,a,s')*(R(s,a,s') + gamma*Vk(s'))]

V*1 = max(a) [0.5 *(0+0.5*0) 0.5 *(0+0.5*0) 0   *(0+0.5*0) 0   *(0+0.5*0) 0   *(1+0.5*0)]
             [0.25*(0+0.5*0) 0.5 *(0+0.5*0) 0.25*(0+0.5*0) 0   *(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0.25*(0+0.5*0) 0.5 *(0+0.5*0) 0.25*(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0   *(0+0.5*0) 0.25*(0+0.5*0) 0.5 *(0+0.5*0) 0.25*(1+0.5*0)]
             [0   *(0+0.5*0) 0.5 *(0+0.5*0) 0   *(0+0.5*0) 0.5 *(0+0.5*0) 0.5 *(1+0.5*0)]

             [0.5 *(0+0.5*0) 0.5 *(0+0.5*0) 0   *(0+0.5*0) 0   *(0+0.5*0) 0   *(1+0.5*0)]
             [0.33*(0+0.5*0) 0.67*(0+0.5*0) 0   *(0+0.5*0) 0   *(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0.33*(0+0.5*0) 0.67*(0+0.5*0) 0   *(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0   *(0+0.5*0) 0.33*(0+0.5*0) 0.67*(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0   *(0+0.5*0) 0   *(0+0.5*0) 0.33*(0+0.5*0) 0.67*(1+0.5*0)]

             [0.67*(0+0.5*0) 0.33*(0+0.5*0) 0   *(0+0.5*0) 0   *(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0.67*(0+0.5*0) 0.33*(0+0.5*0) 0   *(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0   *(0+0.5*0) 0.67*(0+0.5*0) 0.33*(0+0.5*0) 0   *(1+0.5*0)]
             [0   *(0+0.5*0) 0   *(0+0.5*0) 0   *(0+0.5*0) 0.67*(0+0.5*0) 0.33*(1+0.5*0)]
             [0   *(0+0.5*0) 0   *(0+0.5*0) 0   *(0+0.5*0) 0.5 *(0+0.5*0) 0.5 *(1+0.5*0)]

             [0
              0
              0
              0.25
              0.5]

             [0
              0
              0
              0
              0.67]

             [0
              0
              0
              0.33
              0.5]

V*1 = [0
       0
       0
       0.33
       0.67]

